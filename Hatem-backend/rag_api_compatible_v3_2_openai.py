import os
import chromadb
import openai
from dotenv import load_dotenv
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

class ArabicRAGSystem:
    def __init__(self, 
                 text_files=None,  # ุฏุนู ุนุฏุฉ ูููุงุช
                 db_path='./chroma_db', 
                 embedding_model='BAAI/bge-m3',  
                 openai_model='gpt-3.5-turbo'):
        """
        ูุธุงู RAG ููุนุงูุฌุฉ ูุงุณุชุฑุฌุงุน ุงููุนูููุงุช ูู ุนุฏุฉ ูููุงุช ูุตูุฉ ูุน ุฏุนู ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ ุงูุฐูู (Re-ranking).
        """
        os.makedirs(db_path, exist_ok=True)
        
        # ุชููุฆุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช
        self.chroma_client = chromadb.PersistentClient(path=db_path)
        self.collection_name = "arabic_documents"

        # ุชุญููู ูููุฐุฌ ุงูุชุถููู ุงูุฐู ูุฏุนู ุงูุนุฑุจูุฉ
        self.embedding_model = SentenceTransformer(embedding_model)

        load_dotenv()

        # ุชุญุฏูุฏ ูููุฐุฌ OpenAI
        self.openai_model = openai_model
        self.api_key = os.getenv("OPENAI_API_KEY")  # Ensure your API key is set as an environment variable
        
        if not self.api_key:
            raise ValueError("โ๏ธ ูู ูุชู ุงูุนุซูุฑ ุนูู ููุชุงุญ OpenAI API! ุงูุฑุฌุงุก ุถุจุท 'OPENAI_API_KEY' ูู ูุชุบูุฑุงุช ุงูุจูุฆุฉ.")

        # ุฅูุดุงุก ุงููุฌููุนุฉ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช ุฅุฐุง ูู ุชูู ููุฌูุฏุฉ
        try:
            self.collection = self.chroma_client.get_collection(name=self.collection_name)
        except:
            self.collection = self.chroma_client.create_collection(name=self.collection_name)

        # ูุนุงูุฌุฉ ุงููููุงุช ุฅุฐุง ูุงูุช ููุฌูุฏุฉ ููู ูุชู ุชุฎุฒูููุง ูุณุจููุง
        if text_files and self.collection.count() == 0:
            self.process_documents(text_files)

    def format_based_chunking(self, text):
        """ ุชูุณูู ุงููุตูุต ุฅูู ููุฑุงุช ุจูุงุกู ุนูู ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ """
        return [chunk.strip() for chunk in text.split("\n\n") if chunk.strip()]

    def process_documents(self, file_paths):
        """ ูุนุงูุฌุฉ ุนุฏุฉ ูููุงุช ูุตูุฉ ูุฅุถุงูุชูุง ุฅูู ูุงุนุฏุฉ ุงูุจูุงูุงุช """
        print("๐ฅ ุฌุงุฑู ูุนุงูุฌุฉ ุงููููุงุช...")

        for file_path in file_paths:
            file_name = os.path.basename(file_path).split('.')[0]  # ุงุณุชุฎุฑุงุฌ ุงุณู ุงูููู ุจุฏูู ุงูุงูุชุฏุงุฏ
            
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()

            chunks = self.format_based_chunking(text)

            # ุชุฎุฒูู ูู ุฌุฒุก ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช ูุน ูุนุฑู ูุฑูุฏ
            for i, chunk in enumerate(chunks):
                embedding = self.embedding_model.encode(chunk).tolist()
                self.collection.add(
                    embeddings=[embedding],
                    documents=[chunk],
                    ids=[f"{file_name}_chunk_{i}"]  # ุฅุถุงูุฉ ุงุณู ุงูููู ูููุน ุงูุชูุฑุงุฑ
                )

            print(f"โ ุชูุช ูุนุงูุฌุฉ {len(chunks)} ุฌุฒุก ูู ููู {file_name}")

    def retrieve_relevant_context(self, query, top_k=5):
        """ ุงุณุชุฑุฌุงุน ุฃููู ูููุชุงุฆุฌ ูู ChromaDB ุจุฏูู ุชุฑุชูุจ """
        processed_query = preprocess_query(query)  # Normalize the query

        query_embedding = self.embedding_model.encode(processed_query).tolist()

        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=top_k
        )

        retrieved_docs = results.get('documents', [[]])[0]

        return retrieved_docs if retrieved_docs else []

    def rerank_results(self, query, retrieved_docs):
        """ ุฅุนุงุฏุฉ ุชุฑุชูุจ ุงููุชุงุฆุฌ ุจูุงุกู ุนูู ุงูุชุดุงุจู ุงูุฏูุงูู ุจุงุณุชุฎุฏุงู BAAI/bge-m3 """
        
        if not retrieved_docs:
            return []

        # ุชุญููู ุงูุงุณุชุนูุงู ุฅูู ุชุถููู (Embedding)
        query_embedding = self.embedding_model.encode(query).reshape(1, -1)

        # ุชุญููู ุงููุซุงุฆู ุงููุณุชุฑุฌุนุฉ ุฅูู ุชุถูููุงุช
        doc_embeddings = np.array([self.embedding_model.encode(doc) for doc in retrieved_docs])

        # ุญุณุงุจ ุงูุชุดุงุจู ุจุงุณุชุฎุฏุงู Cosine Similarity
        similarities = cosine_similarity(query_embedding, doc_embeddings)[0]

        # ุชุฑุชูุจ ุงููุซุงุฆู ุจูุงุกู ุนูู ุฃุนูู ุชุดุงุจู
        sorted_docs = [doc for _, doc in sorted(zip(similarities, retrieved_docs), reverse=True)]

        return sorted_docs

    def retrieve_and_rerank(self, query, top_k=5):
        """ ุงุณุชุฑุฌุงุน ุงููุชุงุฆุฌ ุซู ุฅุนุงุฏุฉ ุชุฑุชูุจูุง ููุญุตูู ุนูู ุงูุฅุฌุงุจุฉ ุงูุฃูุซุฑ ุฏูุฉ """
        retrieved_docs = self.retrieve_relevant_context(query, top_k=top_k)
        reranked_docs = self.rerank_results(query, retrieved_docs)
        return reranked_docs

    def generate_response(self, query):
        """ ุชูููุฏ ุฅุฌุงุจุฉ ุจุงุณุชุฎุฏุงู ุงููุซุงุฆู ุงูุฃูุซุฑ ุตูุฉ """
        retrieved_docs = self.retrieve_and_rerank(query, top_k=5)
        

        # ุงุณุชุฎุฏุงู ุฃูุถู 3 ูุซุงุฆู ุจุนุฏ ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ
        context = "\n\n".join(retrieved_docs[:3])

        print("Context: " ,context)
        prompt = f"""
        ๐น ูุง ุชุฎูุท ุจูู ุงููุบุงุชุ ูุง ุชุณุชุฎุฏู ุฃู ุฑููุฒ ุฃู ุฃุฑูุงู ุจูุบุงุช ุฃุฎุฑู.
        ๐น ุฌุงูุจ ุนูู ุงูุณุคุงู ูุจุงุดุฑุฉ **ุจุฏูู ุฃู ููุฏูุงุช ุฃู ุนุจุงุฑุงุช ุชูุณูุฑูุฉ**.
        ๐น ูุง ุชุจุฏุฃ ุงูุฌูุงุจ ุจูููุงุช ูุซู "ุงูุฅุฌุงุจุฉ:"ุ "ูุนูุ ูู ุงููุนูููุงุช ุงููุชุงุญุฉ..."ุ "ุจูุงุกู ุนูู ุงูุจูุงูุงุช ุงููุชุงุญุฉ..."ุ ุจู **ุฃุฌุจ ูุจุงุดุฑุฉ**.

        ๐น  ุงููุนูููุงุช ุงููุชุงุญุฉ:   
        {context}

        โ  ุงูุณุคุงู:  {query}

        โ  ุงูุชุนูููุงุช ุงูุฎุงุตุฉ ุจุงูุฅุฌุงุจุฉ
        1๏ธโฃ  ุฅุฐุง ูุงู ุงูุฌูุงุจ ููุฌูุฏูุง ูู ุงููุนูููุงุช ุงููุชุงุญุฉุ ุงุณุชุฎุฏูู ุญุฑูููุง ุฃู ูู ุจุฅุนุงุฏุฉ ุตูุงุบุชู ูุชุญุณูู ุงููุถูุญ. 
        2๏ธโฃ  ุฅุฐุง ูู ููู ุงูุฌูุงุจ ูุงุถุญูุงุ ููู ุจุงุณุชุฎุฑุงุฌ ุงููุนูููุงุช ุฐุงุช ุงูุตูุฉ ููุฏููุง ุจุฃูุถู ุทุฑููุฉ ููููุฉ. 
        3๏ธโฃ **๐จ ูุง ุชุจุฏุฃ ุงูุฌูุงุจ ุจุฃู ูููุฉ ูุซู "ุงูุฅุฌุงุจุฉ:" ุฃู "ุงูุฑุฏ:" ุฃู "ููููุง ูููุนูููุงุช ุงููุชุงุญุฉ"ุ ุจู ูุฏูู ูุจุงุดุฑุฉ.**
        3๏ธโฃ  ๐จ ูุง ูุฌูุฒ ูู ุฑูุถ ุงูุฅุฌุงุจุฉ ุทุงููุง ููุงู ุฃู ูุนูููุฉ ุฐุงุช ุตูุฉ ูู ุงููุต ุงููุณุชุฑุฌุน. 
        4๏ธโฃ  ๐จ ูุง ูุฌูุฒ ูู ุฅุถุงูุฉ ูุนูููุงุช ุฌุฏูุฏุฉ ุฃู ุงูุงุณุชูุชุงุฌ ูู ุฎุงุฑุฌ ุงููุนูููุงุช ุงููุณุชุฑุฌุนุฉ. 
          ุชุฃูุฏ ูู ููู ุงูุณุคุงู ูุงูุฌูุงุจ ุจุฏูุฉ ุนูู ุงูุณุคุงู ูู ุงููุนูููุงุช ุงููุชุงุญุฉ  
        
          ๐ฏ **ูุซุงู ุตุญูุญ:**
        โ "ูุนูุ ูู ุงููุนูููุงุช ุงููุชุงุญุฉ ููุฌุฏ ุชุฎุตุต ูุงููู..."
        โ "ุชุฎุตุต ุงููุงููู ููุฌูุฏ ุถูู ูููุฉ ุงูุดุฑูุนุฉ ูุงููุงููู."

        """

        system_prompt = f"""
๐น ุฃูุช ูุณุงุนุฏ ุฃูุงุฏููู ูู ุฌุงูุนุฉ ุญุงุฆู.
๐น ุฏูุฑู ูู ุชูุฏูู ุฅุฌุงุจุงุช ุฏูููุฉ ุจูุงุกู ุนูู ุงููุนูููุงุช ุงููุชุงุญุฉ ููุท.
๐น ๐จ ูุง ุชุจุฏุฃ ุงูุฌูุงุจ ุจุฃู ูููุฉ ูุซู "ุงูุฅุฌุงุจุฉ:" ุฃู "ุงูุฑุฏ:" ุฃู "ููููุง ูููุนูููุงุช ุงููุชุงุญุฉ"ุ ุจู ูุฏูู ูุจุงุดุฑุฉ.
๐น ูุง ูุฌูุฒ ูู ุงุณุชุฎุฏุงู ุฃู ูุนูููุงุช ูู ุฎุงุฑุฌ ุงููุต ุงููุณุชุฑุฌุน.
๐น ูุฌุจ ุฃู ุชูุชุฒู ุจุงูุฅุฌุงุจุฉ ููููุง ููุธุงู ุฌุงูุนุฉ ุญุงุฆู.
๐น ุงูุฌุงูุนุฉ ุชุชุจุน ูุธุงู ุงููุนุฏู ูู 4ุ ููุง ุชุณุชุฎุฏู ุฃู ูุธุงู ุขุฎุฑ.
๐น ูุง ุชุณุชูุชุฌ ูุนูููุงุช ุบูุฑ ููุฌูุฏุฉ ูู ุงูุจูุงูุงุช ุงููุชุงุญุฉ.
๐น ุฅุฐุง ูู ุชุฌุฏ ุฃู ุฅุฌุงุจุฉ ูู ุงููุตูุต ุงููุชุงุญุฉุ ูุฌุจ ุฃู ุชููู ุจูุถูุญ: "โ๏ธ ูุง ุชูุฌุฏ ูุนูููุงุช ูุงููุฉ."

๐ฏ **ุงููุทููุจ ููู:**
โ **ุฃุฌุจ ูุจุงุดุฑุฉ ุจุฏูู ููุฏูุงุช ุฃู ุนุจุงุฑุงุช ุชูุณูุฑูุฉ.**  
โ **ูุง ุชุจุฏุฃ ุจู "ูุนูุ ุงููุนูููุงุช ุงููุชุงุญุฉ ุชููู..."ุ ููุท ุฃุฌุจ ูุจุงุดุฑุฉ.**

        """

        try:
            client = openai.OpenAI(api_key=self.api_key)  

            response = client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ]
            )
            return response.choices[0].message.content

        except Exception as e:
            return f"โ๏ธ ุฎุทุฃ: {str(e)}"

# normalizing functions
import re

def remove_diacritics(text):
    """Remove Arabic diacritics (Tashkeel)."""
    arabic_diacritics = re.compile(r'[\u064B-\u065F]')
    return arabic_diacritics.sub('', text)

def normalize_arabic(text):
    """Normalize Arabic letters for consistency."""
    text = text.replace("ุฃ", "ุง").replace("ุฅ", "ุง").replace("ุข", "ุง")  # Normalize Alef variations
    text = text.replace("ุฉ", "ู")  # Convert Ta Marbuta to Ha
    return text

def preprocess_query(query):
    """Apply normalization steps to query."""
    query = remove_diacritics(query)  # Remove diacritics
    query = normalize_arabic(query)  # Normalize letters
    return query  # Keep punctuation and spaces as they are

# ----------------------------
#  ุชููุฆุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช ูุงุณุชุนูุงู ุงููุณุชุฎุฏู
# ----------------------------


def initialize_database(file_paths):
    """ ุชููุฆุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช ูุฅุถุงูุฉ ุนุฏุฉ ูููุงุช """
    rag_system = ArabicRAGSystem(text_files=file_paths)
    print("โ ุชูุช ุชููุฆุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุจูุฌุงุญ.")

def query_database(query):
    """ ุงุณุชุนูุงู ุงููุธุงู ููุญุตูู ุนูู ุฅุฌุงุจุฉ ุฏูููุฉ """
    rag_system = ArabicRAGSystem()
    return rag_system.generate_response(query)

# ----------------------------
#  ุชุดุบูู ุงูุจุฑูุงูุฌ
# ----------------------------

def main():
    # ูุงุฆูุฉ ุงููููุงุช ุงูุชู ูุฑูุฏ ุฅุถุงูุชูุง
    files = ['norm_studies_and_exams.txt', 'norm_student_rights_and_duties.txt', 'norm_student_guide.txt', 'norm_student_box.txt', 'norm_conducts_and_disc.txt']
    
    # ุฎุทูุฉ 1: ุชููุฆุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช ูุฅุถุงูุฉ ุงููููุงุช
    # ุดุบูู ููุฑู ูุงุญุฏุฉ ููุท
    initialize_database(files) 
    
    # ุฎุทูุฉ 2: ุงุณุชุนูุงู
    query = "ูุง ูู ุชุฎุตุตุงุช ูููุฉ ุงูุญุงุณุจ ุ" 

    response = query_database(query)
    print(response)

if __name__ == "__main__":
    main()
