import os
import chromadb
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re
import openai
from dotenv import load_dotenv

load_dotenv()


class ArabicRAGSystem:
    def __init__(self, 
                 text_files=None,  
                 db_path='./chroma_db', 
                 embedding_model='BAAI/bge-m3',  
                 openai_model='gpt-3.5-turbo'):
        """
        Ù†Ø¸Ø§Ù… RAG Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ© Ù…Ø¹ Ø¯Ø¹Ù… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø°ÙƒÙŠ (Re-ranking).
        """
        os.makedirs(db_path, exist_ok=True)
        
        # ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.chroma_client = chromadb.PersistentClient(path=db_path)
        self.collection_name = "arabic_documents"

        # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø°ÙŠ ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.embedding_model = SentenceTransformer(embedding_model)
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†Ù…ÙˆØ°Ø¬ OpenAI
        self.openai_model = openai_model
        self.api_key = os.getenv("OPENAI_API_KEY")  # Ensure your API key is set as an environment variable
        
        if not self.api_key:
            raise ValueError("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ OpenAI API! Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¶Ø¨Ø· 'OPENAI_API_KEY' ÙÙŠ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©.")

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
        try:
            self.collection = self.chroma_client.get_collection(name=self.collection_name)
        except:
            self.collection = self.chroma_client.create_collection(name=self.collection_name)

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆÙ„Ù… ÙŠØªÙ… ØªØ®Ø²ÙŠÙ†Ù‡Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§
        if text_files and self.collection.count() == 0:
            self.process_documents(text_files)

    def format_based_chunking(self, text):
        """ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙÙ‚Ø±Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© """
        return [chunk.strip() for chunk in text.split("\n\n") if chunk.strip()]

    def process_documents(self, file_paths):
        """ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ© ÙˆØ¥Ø¶Ø§ÙØªÙ‡Ø§ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª """
        print("ğŸ“¥ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª...")

        for file_path in file_paths:
            file_name = os.path.basename(file_path).split('.')[0]  
            
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()

            chunks = self.format_based_chunking(text)

            # ØªØ®Ø²ÙŠÙ† ÙƒÙ„ Ø¬Ø²Ø¡ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯
            for i, chunk in enumerate(chunks):
                embedding = self.embedding_model.encode(chunk).tolist()
                self.collection.add(
                    embeddings=[embedding],
                    documents=[chunk],
                    ids=[f"{file_name}_chunk_{i}"]
                )

            print(f"âœ… ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© {len(chunks)} Ø¬Ø²Ø¡ Ù…Ù† Ù…Ù„Ù {file_name}")

    def retrieve_relevant_context(self, query, top_k=5):
        """ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£ÙˆÙ„ÙŠ Ù„Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† ChromaDB """
        processed_query = preprocess_query(query)
        query_embedding = self.embedding_model.encode(processed_query).tolist()

        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=top_k
        )

        retrieved_docs = results.get('documents', [[]])[0]

        return retrieved_docs if retrieved_docs else []

    def rerank_results(self, query, retrieved_docs):
        """ Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ """
        if not retrieved_docs:
            return []

        query_embedding = self.embedding_model.encode(query).reshape(1, -1)
        doc_embeddings = np.array([self.embedding_model.encode(doc) for doc in retrieved_docs])
        similarities = cosine_similarity(query_embedding, doc_embeddings)[0]

        sorted_docs = [doc for _, doc in sorted(zip(similarities, retrieved_docs), reverse=True)]
        return sorted_docs

    def retrieve_and_rerank(self, query, top_k=5):
        """ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø«Ù… Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨Ù‡Ø§ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© """
        retrieved_docs = self.retrieve_relevant_context(query, top_k=top_k)
        return self.rerank_results(query, retrieved_docs)

    def generate_response(self, query):
        """ ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© """
        retrieved_docs = self.retrieve_and_rerank(query, top_k=5)

        context = "\n\n".join(retrieved_docs[:3])
        print("Context: ", context)

        prompt = f"""
        ğŸ”¹ Ù„Ø§ ØªØ®Ù„Ø· Ø¨ÙŠÙ† Ø§Ù„Ù„ØºØ§ØªØŒ Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ø±Ù…ÙˆØ² Ø£Ùˆ Ø£Ø±Ù‚Ø§Ù… Ø¨Ù„ØºØ§Øª Ø£Ø®Ø±Ù‰.
        ğŸ”¹ Ø¬Ø§ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© **Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù…Ù‚Ø¯Ù…Ø§Øª Ø£Ùˆ Ø¹Ø¨Ø§Ø±Ø§Øª ØªÙØ³ÙŠØ±ÙŠØ©**.
        ğŸ”¹ Ù„Ø§ ØªØ¨Ø¯Ø£ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø¨ÙƒÙ„Ù…Ø§Øª Ù…Ø«Ù„ "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"ØŒ "Ù†Ø¹Ù…ØŒ ÙÙŠ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©..."ØŒ "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©..."ØŒ Ø¨Ù„ **Ø£Ø¬Ø¨ Ù…Ø¨Ø§Ø´Ø±Ø©**.

        ğŸ”¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:   
        {context}

        â“ Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

        âœ… Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
        1ï¸âƒ£ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ø­Ø±ÙÙŠÙ‹Ø§ Ø£Ùˆ Ø£Ø¹Ø¯ ØµÙŠØ§ØºØªÙ‡Ø§ Ø¨ÙˆØ¶ÙˆØ­.
        2ï¸âƒ£ Ù„Ø§ ØªØ¨Ø¯Ø£ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø¨Ø£ÙŠ ÙƒÙ„Ù…Ø© Ù…Ø«Ù„ "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:" Ø£Ùˆ "Ø§Ù„Ø±Ø¯:".
        3ï¸âƒ£ ğŸš¨ Ù„Ø§ ÙŠØ¬ÙˆØ² Ù„Ùƒ Ø±ÙØ¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø·Ø§Ù„Ù…Ø§ Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø© Ø°Ø§Øª ØµÙ„Ø©.
        4ï¸âƒ£ ğŸš¨ Ù„Ø§ ØªØ¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø£Ùˆ ØªØ³ØªÙ†ØªØ¬ Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªØ§Ø­Ø©.

        ğŸ¯ **Ù…Ø«Ø§Ù„ ØµØ­ÙŠØ­:**
        âŒ "Ù†Ø¹Ù…ØŒ ÙÙŠ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙŠÙˆØ¬Ø¯ ØªØ®ØµØµ Ù‚Ø§Ù†ÙˆÙ†..."
        âœ… "ØªØ®ØµØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯ Ø¶Ù…Ù† ÙƒÙ„ÙŠØ© Ø§Ù„Ø´Ø±ÙŠØ¹Ø© ÙˆØ§Ù„Ù‚Ø§Ù†ÙˆÙ†."
        """

        system_prompt = f"""
ğŸ”¹ Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ ÙÙŠ Ø¬Ø§Ù…Ø¹Ø© Ø­Ø§Ø¦Ù„.
ğŸ”¹ Ø¯ÙˆØ±Ùƒ Ù‡Ùˆ ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙ‚Ø·.
ğŸ”¹ ğŸš¨ Ù„Ø§ ØªØ¨Ø¯Ø£ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø¨Ø£ÙŠ ÙƒÙ„Ù…Ø© Ù…Ø«Ù„ "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:" Ø£Ùˆ "Ø§Ù„Ø±Ø¯:" Ø£Ùˆ "ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"ØŒ Ø¨Ù„ Ù‚Ø¯Ù…Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø©.
ğŸ”¹ Ù„Ø§ ÙŠØ¬ÙˆØ² Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹.
ğŸ”¹ ÙŠØ¬Ø¨ Ø£Ù† ØªÙ„ØªØ²Ù… Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆÙÙ‚Ù‹Ø§ Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù…Ø¹Ø© Ø­Ø§Ø¦Ù„.
ğŸ”¹ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ØªØªØ¨Ø¹ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¹Ø¯Ù„ Ù…Ù† 4ØŒ ÙÙ„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ù†Ø¸Ø§Ù… Ø¢Ø®Ø±.
ğŸ”¹ Ù„Ø§ ØªØ³ØªÙ†ØªØ¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.
ğŸ”¹ Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø£ÙŠ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚ÙˆÙ„ Ø¨ÙˆØ¶ÙˆØ­: "âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©."
"""
        
        try:
            client = openai.OpenAI(api_key=self.api_key)  

            response = client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ]
            )
            return response.choices[0].message.content

        except Exception as e:
            return f"âš ï¸ Ø®Ø·Ø£: {str(e)}"

# ----------------------------
#  Ø¯ÙˆØ§Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ
# ----------------------------

def remove_diacritics(text):
    """ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ """
    arabic_diacritics = re.compile(r'[\u064B-\u065F]')
    return arabic_diacritics.sub('', text)

def normalize_arabic(text):
    """ ØªÙˆØ­ÙŠØ¯ Ø´ÙƒÙ„ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© """
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    text = text.replace("Ø©", "Ù‡")
    return text

def preprocess_query(query):
    """ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª """
    query = remove_diacritics(query)
    query = normalize_arabic(query)
    return query

# ----------------------------
#  ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
# ----------------------------

def main():
    files = ['norm_studies_and_exams.txt', 'norm_student_rights_and_duties.txt']
    rag_system = ArabicRAGSystem(text_files=files) 
    query = "Ù…Ø§ Ù‡ÙŠ ØªØ®ØµØµØ§Øª ÙƒÙ„ÙŠØ© Ø§Ù„Ø­Ø§Ø³Ø¨ØŸ"
    response = rag_system.generate_response(query)
    print(response)

if __name__ == "__main__":
    main()
