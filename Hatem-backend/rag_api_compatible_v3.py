import os
import chromadb
import ollama
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

class ArabicRAGSystem:
    def __init__(self, 
                 text_files=None,  # Ø¯Ø¹Ù… Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª
                 db_path='./chroma_db', 
                 embedding_model='BAAI/bge-m3',  
                 ollama_model='qwen2'):
        """
        Ù†Ø¸Ø§Ù… RAG Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ© Ù…Ø¹ Ø¯Ø¹Ù… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø°ÙƒÙŠ (Re-ranking).
        """
        os.makedirs(db_path, exist_ok=True)
        
        # ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.chroma_client = chromadb.PersistentClient(path=db_path)
        self.collection_name = "arabic_documents"

        # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø°ÙŠ ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.embedding_model = SentenceTransformer(embedding_model)
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†Ù…ÙˆØ°Ø¬ Ollama
        self.ollama_model = ollama_model
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
        try:
            self.collection = self.chroma_client.get_collection(name=self.collection_name)
        except:
            self.collection = self.chroma_client.create_collection(name=self.collection_name)

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø© ÙˆÙ„Ù… ÙŠØªÙ… ØªØ®Ø²ÙŠÙ†Ù‡Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§
        if text_files and self.collection.count() == 0:
            self.process_documents(text_files)

    def format_based_chunking(self, text):
        """ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ÙÙ‚Ø±Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© """
        return [chunk.strip() for chunk in text.split("\n\n") if chunk.strip()]

    def process_documents(self, file_paths):
        """ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ© ÙˆØ¥Ø¶Ø§ÙØªÙ‡Ø§ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª """
        print("ğŸ“¥ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª...")

        for file_path in file_paths:
            file_name = os.path.basename(file_path).split('.')[0]  # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯
            
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()

            chunks = self.format_based_chunking(text)

            # ØªØ®Ø²ÙŠÙ† ÙƒÙ„ Ø¬Ø²Ø¡ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯
            for i, chunk in enumerate(chunks):
                embedding = self.embedding_model.encode(chunk).tolist()
                self.collection.add(
                    embeddings=[embedding],
                    documents=[chunk],
                    ids=[f"{file_name}_chunk_{i}"]  # Ø¥Ø¶Ø§ÙØ© Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±
                )

            print(f"âœ… ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© {len(chunks)} Ø¬Ø²Ø¡ Ù…Ù† Ù…Ù„Ù {file_name}")

    def retrieve_relevant_context(self, query, top_k=10):
        """ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£ÙˆÙ„ÙŠ Ù„Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† ChromaDB Ø¨Ø¯ÙˆÙ† ØªØ±ØªÙŠØ¨ """
        processed_query = preprocess_query(query)  # Normalize the query

        query_embedding = self.embedding_model.encode(processed_query).tolist()

        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=top_k
        )

        retrieved_docs = results.get('documents', [[]])[0]

        return retrieved_docs if retrieved_docs else []

    def rerank_results(self, query, retrieved_docs):
        """ Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BAAI/bge-m3 """
        
        if not retrieved_docs:
            return []

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† (Embedding)
        query_embedding = self.embedding_model.encode(query).reshape(1, -1)

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ†Ø§Øª
        doc_embeddings = np.array([self.embedding_model.encode(doc) for doc in retrieved_docs])

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Cosine Similarity
        similarities = cosine_similarity(query_embedding, doc_embeddings)[0]

        # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡
        sorted_docs = [doc for _, doc in sorted(zip(similarities, retrieved_docs), reverse=True)]

        return sorted_docs

    def retrieve_and_rerank(self, query, top_k=10):
        """ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø«Ù… Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨Ù‡Ø§ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© """
        retrieved_docs = self.retrieve_relevant_context(query, top_k=top_k)
        reranked_docs = self.rerank_results(query, retrieved_docs)
        return reranked_docs

    def generate_response(self, query):
        """ ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© """
        retrieved_docs = self.retrieve_and_rerank(query, top_k=10)
        

        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„ 3 ÙˆØ«Ø§Ø¦Ù‚ Ø¨Ø¹Ø¯ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨
        context = "\n\n".join(retrieved_docs[:3])

        print("Context: " ,context)
        prompt = f"""
        ğŸ”¹ Ù„Ø§ ØªØ®Ù„Ø· Ø¨ÙŠÙ† Ø§Ù„Ù„ØºØ§ØªØŒ Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ø±Ù…ÙˆØ² Ø£Ùˆ Ø£Ø±Ù‚Ø§Ù… Ø¨Ù„ØºØ§Øª Ø£Ø®Ø±Ù‰.

        ğŸ”¹ **Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:**  
        {context}

        â“ **Ø§Ù„Ø³Ø¤Ø§Ù„:** {query}

        âœ… **Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:**
        1ï¸âƒ£ **Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¬ÙˆØ§Ø¨ Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§ ÙÙŠ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡ Ø­Ø±ÙÙŠÙ‹Ø§ Ø£Ùˆ Ù‚Ù… Ø¨Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØªÙ‡ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØ¶ÙˆØ­.**
        2ï¸âƒ£ **Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ø¬ÙˆØ§Ø¨ ÙˆØ§Ø¶Ø­Ù‹Ø§ØŒ ÙÙ‚Ù… Ø¨Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© ÙˆÙ‚Ø¯Ù…Ù‡Ø§ Ø¨Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ù…Ù…ÙƒÙ†Ø©.**
        3ï¸âƒ£ **ğŸš¨ Ù„Ø§ ÙŠØ¬ÙˆØ² Ù„Ùƒ Ø±ÙØ¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø·Ø§Ù„Ù…Ø§ Ù‡Ù†Ø§Ùƒ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø© Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹.**
        4ï¸âƒ£ **ğŸš¨ Ù„Ø§ ÙŠØ¬ÙˆØ² Ù„Ùƒ Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø£Ùˆ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©.**
        ** ØªØ£ÙƒØ¯ Ù…Ù† ÙÙ‡Ù… Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØ§Ù„Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© **

        """

        system_prompt = f"""
ğŸ”¹ **Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ ÙÙŠ Ø¬Ø§Ù…Ø¹Ø© Ø­Ø§Ø¦Ù„.** 
ğŸ”¹ Ø¯ÙˆØ±Ùƒ Ù‡Ùˆ ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙ‚Ø·.
ğŸ”¹ Ù„Ø§ ÙŠØ¬ÙˆØ² Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹.
ğŸ”¹ ÙŠØ¬Ø¨ Ø£Ù† ØªÙ„ØªØ²Ù… Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆÙÙ‚Ù‹Ø§ Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù…Ø¹Ø© Ø­Ø§Ø¦Ù„.
ğŸ”¹ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ØªØªØ¨Ø¹ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¹Ø¯Ù„ Ù…Ù† 4ØŒ ÙÙ„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ù†Ø¸Ø§Ù… Ø¢Ø®Ø±.
ğŸ”¹ Ù„Ø§ ØªØ³ØªÙ†ØªØ¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.
ğŸ”¹ Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø£ÙŠ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚ÙˆÙ„ Ø¨ÙˆØ¶ÙˆØ­: "âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©."
        """

        try:
            response = ollama.chat(
                model=self.ollama_model,
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': prompt}
                ]
            )
            return response['message']['content']

        except Exception as e:
            return f"âš ï¸ Ø®Ø·Ø£: {str(e)}"

# normalizing functions
import re

def remove_diacritics(text):
    """Remove Arabic diacritics (Tashkeel)."""
    arabic_diacritics = re.compile(r'[\u064B-\u065F]')
    return arabic_diacritics.sub('', text)

def normalize_arabic(text):
    """Normalize Arabic letters for consistency."""
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")  # Normalize Alef variations
    text = text.replace("Ø©", "Ù‡")  # Convert Ta Marbuta to Ha
    return text

def preprocess_query(query):
    """Apply normalization steps to query."""
    query = remove_diacritics(query)  # Remove diacritics
    query = normalize_arabic(query)  # Normalize letters
    return query  # Keep punctuation and spaces as they are

# ----------------------------
#  ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
# ----------------------------


def initialize_database(file_paths):
    """ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ø¶Ø§ÙØ© Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª """
    rag_system = ArabicRAGSystem(text_files=file_paths)
    print("âœ… ØªÙ…Øª ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")

def query_database(query):
    """ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© """
    rag_system = ArabicRAGSystem()
    return rag_system.generate_response(query)

# ----------------------------
#  ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬
# ----------------------------

def main():
    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ Ù†Ø±ÙŠØ¯ Ø¥Ø¶Ø§ÙØªÙ‡Ø§
    files = ['norm_studies_and_exams.txt', 'norm_student_rights_and_duties.txt', 'norm_student_guide.txt', 'norm_student_box.txt', 'norm_conducts_and_disc.txt']
    
    # Ø®Ø·ÙˆØ© 1: ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù„ÙØ§Øª
    initialize_database(files) 
    
    # Ø®Ø·ÙˆØ© 2: Ø§Ø³ØªØ¹Ù„Ø§Ù…
    query = "Ù…Ø§ Ù‡ÙŠ ØªØ®ØµØµØ§Øª ÙƒÙ„ÙŠØ© Ø§Ù„Ø­Ø§Ø³Ø¨ ØŸ" 

    response = query_database(query)
    print(response)

if __name__ == "__main__":
    main()
